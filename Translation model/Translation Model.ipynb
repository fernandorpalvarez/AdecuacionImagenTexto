{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4e53825",
   "metadata": {},
   "source": [
    "# CLEANING DATASET\n",
    "\n",
    "Dataset selected from: http://www.manythings.org/anki/ \\\n",
    "Tutorial: https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9250cd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental.preprocessing'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c20e1676de40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     raise ImportError(\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "from numpy import argmax\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3755ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e6fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and saving the dataset (execute this once)\n",
    "# filename = \"data/spa.txt\"\n",
    "# doc = load_doc(filename)\n",
    "# pairs = to_pairs(doc)\n",
    "# clean_pairs = clean_pairs(pairs)\n",
    "# save_clean_data(clean_pairs, 'data/english-spanish.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc77f6a",
   "metadata": {},
   "source": [
    "# SPLITING DATASET IN TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b264f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a021ce39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128084, 3)\n",
      "Saved: data/english-spanish-both.pkl\n",
      "Saved: data/english-spanish-train.pkl\n",
      "Saved: data/english-spanish-test.pkl\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('data/english-spanish.pkl')\n",
    " \n",
    "print(raw_dataset.shape)\n",
    "    \n",
    "# dataset size\n",
    "n_sentences = 40000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:39000], dataset[39000:]\n",
    "# save\n",
    "save_clean_data(dataset, 'data/english-spanish-both.pkl')\n",
    "save_clean_data(train, 'data/english-spanish-train.pkl')\n",
    "save_clean_data(test, 'data/english-spanish-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1287cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c24a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('data/english-spanish-both.pkl')\n",
    "train = load_clean_sentences('data/english-spanish-train.pkl')\n",
    "test = load_clean_sentences('data/english-spanish-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9fe1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad054ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 5893\n",
      "English Max Length: 7\n",
      "Spanish Vocabulary Size: 11178\n",
      "Spanish Max Length: 15\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare spanish tokenizer\n",
    "esp_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "esp_vocab_size = len(esp_tokenizer.word_index) + 1\n",
    "esp_length = max_length(dataset[:, 1])\n",
    "print('Spanish Vocabulary Size: %d' % esp_vocab_size)\n",
    "print('Spanish Max Length: %d' % (esp_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d9879",
   "metadata": {},
   "source": [
    "# TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18b13d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    del lines\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    del ylist\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70249704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 3\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66e7f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 4\n",
    "# define NMT model\n",
    "# def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "#     model.add(LSTM(n_units))\n",
    "#     model.add(RepeatVector(tar_timesteps))\n",
    "#     model.add(LSTM(n_units))\n",
    "#     model.add(RepeatVector(tar_timesteps))\n",
    "#     model.add(LSTM(n_units, return_sequences=True))\n",
    "#     model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "277d6401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 15, 256)           2861568   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 7, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 7, 5893)           1514501   \n",
      "=================================================================\n",
      "Total params: 5,426,693\n",
      "Trainable params: 5,426,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(esp_vocab_size, eng_vocab_size, esp_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5415c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(esp_tokenizer, esp_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(esp_tokenizer, esp_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4459ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "610/610 - 101s - loss: 3.7663 - val_loss: 3.4487\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.44874, saving model to models\\model4.h5\n",
      "Epoch 2/30\n",
      "610/610 - 98s - loss: 3.3674 - val_loss: 3.3099\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.44874 to 3.30992, saving model to models\\model4.h5\n",
      "Epoch 3/30\n",
      "610/610 - 99s - loss: 3.1523 - val_loss: 3.1225\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.30992 to 3.12253, saving model to models\\model4.h5\n",
      "Epoch 4/30\n",
      "610/610 - 99s - loss: 2.9889 - val_loss: 3.0369\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.12253 to 3.03691, saving model to models\\model4.h5\n",
      "Epoch 5/30\n",
      "610/610 - 100s - loss: 2.8410 - val_loss: 2.8745\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.03691 to 2.87449, saving model to models\\model4.h5\n",
      "Epoch 6/30\n",
      "610/610 - 99s - loss: 2.6199 - val_loss: 2.6736\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.87449 to 2.67359, saving model to models\\model4.h5\n",
      "Epoch 7/30\n",
      "610/610 - 99s - loss: 2.3691 - val_loss: 2.5390\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.67359 to 2.53902, saving model to models\\model4.h5\n",
      "Epoch 8/30\n",
      "610/610 - 98s - loss: 2.1724 - val_loss: 2.4030\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.53902 to 2.40297, saving model to models\\model4.h5\n",
      "Epoch 9/30\n",
      "610/610 - 99s - loss: 1.9961 - val_loss: 2.2938\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.40297 to 2.29384, saving model to models\\model4.h5\n",
      "Epoch 10/30\n",
      "610/610 - 99s - loss: 1.8269 - val_loss: 2.2136\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.29384 to 2.21365, saving model to models\\model4.h5\n",
      "Epoch 11/30\n",
      "610/610 - 98s - loss: 1.6672 - val_loss: 2.1082\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.21365 to 2.10821, saving model to models\\model4.h5\n",
      "Epoch 12/30\n",
      "610/610 - 97s - loss: 1.5227 - val_loss: 2.0501\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.10821 to 2.05010, saving model to models\\model4.h5\n",
      "Epoch 13/30\n",
      "610/610 - 96s - loss: 1.3919 - val_loss: 1.9981\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.05010 to 1.99813, saving model to models\\model4.h5\n",
      "Epoch 14/30\n",
      "610/610 - 96s - loss: 1.2778 - val_loss: 1.9495\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.99813 to 1.94951, saving model to models\\model4.h5\n",
      "Epoch 15/30\n",
      "610/610 - 95s - loss: 1.1597 - val_loss: 1.9269\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.94951 to 1.92691, saving model to models\\model4.h5\n",
      "Epoch 16/30\n",
      "610/610 - 96s - loss: 1.0586 - val_loss: 1.8964\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.92691 to 1.89643, saving model to models\\model4.h5\n",
      "Epoch 17/30\n",
      "610/610 - 95s - loss: 0.9657 - val_loss: 1.8720\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.89643 to 1.87196, saving model to models\\model4.h5\n",
      "Epoch 18/30\n",
      "610/610 - 96s - loss: 0.8815 - val_loss: 1.8534\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.87196 to 1.85342, saving model to models\\model4.h5\n",
      "Epoch 19/30\n",
      "610/610 - 95s - loss: 0.8062 - val_loss: 1.8621\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.85342\n",
      "Epoch 20/30\n",
      "610/610 - 96s - loss: 0.7364 - val_loss: 1.8407\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.85342 to 1.84066, saving model to models\\model4.h5\n",
      "Epoch 21/30\n",
      "610/610 - 96s - loss: 0.6738 - val_loss: 1.8240\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.84066 to 1.82396, saving model to models\\model4.h5\n",
      "Epoch 22/30\n",
      "610/610 - 95s - loss: 0.6173 - val_loss: 1.8474\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.82396\n",
      "Epoch 23/30\n",
      "610/610 - 97s - loss: 0.5678 - val_loss: 1.8360\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.82396\n",
      "Epoch 24/30\n",
      "610/610 - 96s - loss: 0.5222 - val_loss: 1.8591\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.82396\n",
      "Epoch 25/30\n",
      "610/610 - 97s - loss: 0.4783 - val_loss: 1.8676\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.82396\n",
      "Epoch 26/30\n",
      "610/610 - 100s - loss: 0.4398 - val_loss: 1.8869\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.82396\n",
      "Epoch 27/30\n",
      "610/610 - 95s - loss: 0.4079 - val_loss: 1.8715\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.82396\n",
      "Epoch 28/30\n",
      "610/610 - 95s - loss: 0.3788 - val_loss: 1.9160\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.82396\n",
      "Epoch 29/30\n",
      "610/610 - 96s - loss: 0.3513 - val_loss: 1.9006\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.82396\n",
      "Epoch 30/30\n",
      "610/610 - 95s - loss: 0.3246 - val_loss: 1.9222\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.82396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21937247948>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "filename = 'models/model3.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1015a937",
   "metadata": {},
   "source": [
    "# EVALUATING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e06617e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('data/english-spanish-both.pkl')\n",
    "train = load_clean_sentences('data/english-spanish-train.pkl')\n",
    "test = load_clean_sentences('data/english-spanish-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "# prepare spanish tokenizer\n",
    "esp_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "esp_vocab_size = len(esp_tokenizer.word_index) + 1\n",
    "esp_length = max_length(dataset[:, 1])\n",
    "\n",
    "# prepare data\n",
    "trainX = encode_sequences(esp_tokenizer, esp_length, train[:, 1])\n",
    "testX = encode_sequences(esp_tokenizer, esp_length, test[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a51a26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c4736ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src, x = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "580f6257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[podemos sentarnos alli], target=[can we sit over there], predicted=[can we sit on later]\n",
      "src=[conoci a tom en australia], target=[i met tom in australia], predicted=[i met tom boston]\n",
      "src=[las mujeres lo adoraron], target=[women loved it], predicted=[women loved it]\n",
      "src=[te gustan las ostras], target=[do you like oysters], predicted=[do you like oysters]\n",
      "src=[salve al gato], target=[i rescued the cat], predicted=[i rescued the cat]\n",
      "src=[era cierta su historia], target=[was her story true], predicted=[all the story true]\n",
      "src=[no me hizo caso], target=[he ignored me], predicted=[it didnt me me]\n",
      "src=[tom solo se encogio de hombros], target=[tom just shrugged], predicted=[tom never shrugged]\n",
      "src=[cuentame acerca de tu hijo], target=[tell me about your son], predicted=[tell me what your team]\n",
      "src=[oye tom abreme], target=[hey tom open up], predicted=[hey tom hurt up]\n",
      "BLEU-1: 0.781127\n",
      "BLEU-2: 0.690336\n",
      "BLEU-3: 0.634418\n",
      "BLEU-4: 0.495131\n",
      "test\n",
      "src=[tom odia las reglas], target=[tom hates the rules], predicted=[tom hates the rules]\n",
      "src=[los poetas escriben poemas], target=[poets write poems], predicted=[the both animals]\n",
      "src=[no confies en extranos], target=[dont trust strangers], predicted=[dont talk in]\n",
      "src=[ha llovido mucho], target=[it rained heavily], predicted=[it it much lot]\n",
      "src=[coma lo que quiera], target=[eat what you want], predicted=[what what do you want]\n",
      "src=[sos el mejor], target=[youre the best], predicted=[youre the the]\n",
      "src=[son tiempos duros], target=[times are tough], predicted=[youre are]\n",
      "src=[tu tienes los pies planos], target=[do you have flat feet], predicted=[your your are dry]\n",
      "src=[odio a los insectos], target=[i hate insects], predicted=[i hate politics]\n",
      "src=[tom todavia esta en la cama], target=[tom is still in bed], predicted=[tom is still of bed]\n",
      "BLEU-1: 0.535519\n",
      "BLEU-2: 0.404155\n",
      "BLEU-3: 0.349067\n",
      "BLEU-4: 0.230316\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('models/model3.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d390f2d",
   "metadata": {},
   "source": [
    "# SELF-CONTAINED FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "184f2dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traductor(model, sentence):\n",
    "    sentence = clean_text(sentence)\n",
    "    sentence = encode_sequences(esp_tokenizer, esp_length, sentence)\n",
    "    for i, source in enumerate(sentence):\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        return predict_sequence(model, eng_tokenizer, source)\n",
    "\n",
    "def clean_text(line):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    clean_pair = list()\n",
    "    # normalize unicode characters\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    # tokenize on white space\n",
    "    line = line.split()\n",
    "    # convert to lowercase\n",
    "    line = [word.lower() for word in line]\n",
    "    # remove punctuation from each token\n",
    "    line = [word.translate(table) for word in line]\n",
    "    # remove non-printable chars form each token\n",
    "    line = [re_print.sub('', w) for w in line]\n",
    "    # remove tokens with numbers in them\n",
    "    line = [word for word in line if word.isalpha()]\n",
    "    # store as string\n",
    "    clean_pair.append(' '.join(line)) \n",
    "    return clean_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47bde070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 256)           2398976   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 7, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 7, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 7, 4889)           1256473   \n",
      "=================================================================\n",
      "Total params: 4,706,073\n",
      "Trainable params: 4,706,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:9 out of the last 40008 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021980BBD558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'theres the brother'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('models/model3.h5')\n",
    "print(model.summary())\n",
    "traductor(model, \"ella le despertó\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
